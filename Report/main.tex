\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Implementation of Neural LDPC Decoders with Degree-Specific Weight Sharing and RCQ Quantization}

\author{\IEEEauthorblockN{LDPC Decoding Implementation}
\IEEEauthorblockA{Based on: LDPC Decoding with Degree-Specific Neural Message Weights and RCQ Decoding\\
arXiv:2310.15483v2 [eess.SP] 5 Dec 2023\\
Linfang Wang, Caleb Terrill, Richard Wesel, and Dariush Divsalar}
}

\maketitle


\begin{abstract}
This paper presents a comprehensive implementation of neural Low-Density Parity-Check (LDPC) decoders based on the recent work by Wang et al. The implementation includes Neural MinSum (N-NMS) decoders with edge-specific weights, Neural 2D MinSum (N-2D-NMS) decoders with node-degree-based weight sharing, Reconstruction-Computation-Quantization (RCQ) decoders, and Weighted RCQ (W-RCQ) decoders. The implementation addresses gradient explosion issues through posterior joint training and provides comprehensive simulation frameworks for performance evaluation. Experimental results demonstrate significant parameter reduction (from 4.8×10⁵ to 8-15 parameters per iteration) while maintaining comparable decoding performance, making the approach suitable for hardware implementation.
\end{abstract}

\begin{IEEEkeywords}
LDPC codes, neural decoders, weight sharing, RCQ quantization, gradient explosion, hardware efficiency
\end{IEEEkeywords}

\section{Introduction}

Low-Density Parity-Check (LDPC) codes have become fundamental in modern communication systems, including wireless communications, satellite communications, and storage systems. Traditional message-passing decoders such as Belief Propagation (BP) and MinSum algorithms provide good performance but are suboptimal due to cycles in the Tanner graph.

Recent advances in neural networks have shown promise in improving LDPC decoder performance by incorporating learnable weights into the message-passing process. However, neural decoders typically require distinct weights for each edge in each iteration, leading to a parameter count proportional to the number of edges in the Tanner graph. This makes neural decoders impractical for long-blocklength LDPC codes due to memory and computational constraints.

This paper presents a comprehensive implementation of the neural LDPC decoding framework proposed by Wang et al. \cite{wang2023ldpc}, which addresses these challenges through:

\begin{itemize}
\item Node-degree-based weight sharing schemes that reduce parameters from O(edges) to O(degrees)
\item Posterior joint training to address gradient explosion issues
\item RCQ quantization for low-bitwidth implementations
\item Weighted RCQ decoders combining neural weights with quantization
\end{itemize}

\section{Background and Related Work}

\subsection{LDPC Codes and Message-Passing Decoders}

LDPC codes are linear block codes defined by a sparse parity-check matrix $H \in \mathbb{F}_2^{(n-k) \times n}$, where $n$ is the codeword length and $k$ is the dataword length. The code rate is $R = k/n$.

Traditional MinSum decoding updates check-to-variable (C2V) messages as:
\begin{equation}
u_{c_i \to v_j}^{(t)} = \beta \cdot \prod_{v_{j'} \in \mathcal{N}(c_i) \setminus \{v_j\}} \text{sgn}(l_{v_{j'} \to c_i}^{(t-1)}) \cdot \min_{v_{j'} \in \mathcal{N}(c_i) \setminus \{v_j\}} |l_{v_{j'} \to c_i}^{(t-1)}|
\end{equation}

where $\beta$ is a normalization factor, $\mathcal{N}(c_i)$ is the set of variable nodes connected to check node $c_i$, and $l_{v_{j'} \to c_i}^{(t-1)}$ are variable-to-check (V2C) messages.

\subsection{Neural LDPC Decoders}

Neural LDPC decoders enhance traditional message-passing by incorporating learnable weights. The Neural MinSum (N-NMS) decoder assigns distinct weights to each edge:

\begin{equation}
u_{c_i \to v_j}^{(t)} = \beta^{(t)}_{(c_i,v_j)} \cdot \prod_{v_{j'} \in \mathcal{N}(c_i) \setminus \{v_j\}} \text{sgn}(l_{v_{j'} \to c_i}^{(t-1)}) \cdot \min_{v_{j'} \in \mathcal{N}(c_i) \setminus \{v_j\}} |l_{v_{j'} \to c_i}^{(t-1)}|
\end{equation}

where $\beta^{(t)}_{(c_i,v_j)}$ are learnable parameters for each edge $(c_i, v_j)$ in iteration $t$.

\section{Proposed Implementation}

\subsection{Neural MinSum Decoder with Edge-Specific Weights}

The Neural MinSum decoder assigns a distinct weight to each edge in each iteration. For an LDPC code with $E$ edges and $T$ iterations, this requires $E \times T$ parameters.

\begin{algorithmic}
\STATE \textbf{Input:} Channel LLRs $\mathbf{l}_{ch}$, Parity check matrix $H$, Max iterations $T$
\STATE Initialize V2C messages: $l_{v_j \to c_i}^{(0)} = l_{ch,j}$ for all edges
\FOR{$t = 1$ to $T$}
    \FOR{each check node $c_i$}
        \FOR{each variable node $v_j \in \mathcal{N}(c_i)$}
            \STATE Compute C2V message with edge-specific weight:
            \STATE $u_{c_i \to v_j}^{(t)} = \beta^{(t)}_{(c_i,v_j)} \cdot \text{MinSum}(l_{v_{j'} \to c_i}^{(t-1)})$
        \ENDFOR
    \ENDFOR
    \FOR{each variable node $v_j$}
        \FOR{each check node $c_i \in \mathcal{N}(v_j)$}
            \STATE Update V2C message: $l_{v_j \to c_i}^{(t)} = l_{ch,j} + \sum_{c_{i'} \in \mathcal{N}(v_j) \setminus \{c_i\}} u_{c_{i'} \to v_j}^{(t)}$
        \ENDFOR
    \ENDFOR
    \IF{all parity checks satisfied}
        \STATE \textbf{return} decoded codeword
    \ENDIF
\ENDFOR
\STATE \textbf{return} final decision
\end{algorithmic}

\subsection{Node-Degree-Based Weight Sharing}

To reduce the number of parameters, we propose weight sharing based on node degrees. Four different sharing schemes are implemented:

\textbf{Type 1:} Same weight for edges with same check node degree AND variable node degree
\begin{equation}
\beta^{(t)}_{(c_i,v_j)} = \beta^{(t)}_{(\deg(c_i), \deg(v_j))}
\end{equation}

\textbf{Type 2:} Separate weights for check node degree and variable node degree
\begin{equation}
\beta^{(t)}_{(c_i,v_j)} = \beta^{(t)}_{\deg(c_i)}, \quad \alpha^{(t)}_{(c_i,v_j)} = \alpha^{(t)}_{\deg(v_j)}
\end{equation}

\textbf{Type 3:} Only check node degree based weights
\begin{equation}
\beta^{(t)}_{(c_i,v_j)} = \beta^{(t)}_{\deg(c_i)}
\end{equation}

\textbf{Type 4:} Only variable node degree based weights
\begin{equation}
\alpha^{(t)}_{(c_i,v_j)} = \alpha^{(t)}_{\deg(v_j)}
\end{equation}

\subsection{Posterior Joint Training}

Gradient explosion is a critical issue in training neural LDPC decoders. The posterior joint training method addresses this by computing gradients using only posterior information:

\begin{equation}
\frac{\partial J}{\partial u_{c_i \to v_j}^{(t)}} = \frac{\partial J}{\partial l_{v_j}^{(t)}}
\end{equation}

This prevents large-magnitude gradients from propagating to preceding layers and stabilizes training.

\subsection{RCQ Decoding}

The Reconstruction-Computation-Quantization (RCQ) decoder uses non-uniform quantization with power function thresholds:

\begin{equation}
\tau_j = C \cdot \left(\frac{j}{2^{b_c-1}-1}\right)^{\gamma}
\end{equation}

where $C$ controls the maximum magnitude, $\gamma$ controls non-uniformity, and $b_c$ is the quantization bit width.

\subsection{Weighted RCQ Decoder}

The Weighted RCQ decoder combines neural weights with RCQ quantization:

\begin{equation}
u_{c_i \to v_j}^{(t)} = \mathcal{Q}^{-1}\left(\mathcal{Q}\left(\beta^{(t)}_{(c_i,v_j)} \cdot \text{MinSum}(l_{v_{j'} \to c_i}^{(t-1)})\right)\right)
\end{equation}

where $\mathcal{Q}$ and $\mathcal{Q}^{-1}$ are the quantization and reconstruction functions.

\section{Implementation Details}

\subsection{Software Architecture}

The implementation consists of six main modules:

\begin{itemize}
\item \texttt{ldpc\_decoder.py}: Basic LDPC decoder and code structure
\item \texttt{neural\_minsum\_decoder.py}: Neural MinSum decoder with edge-specific weights
\item \texttt{neural\_2d\_decoder.py}: Neural 2D MinSum decoder with weight sharing
\item \texttt{rcq\_decoder.py}: RCQ and Weighted RCQ decoders
\item \texttt{training\_framework.py}: Training framework with posterior joint training
\item \texttt{simulation\_framework.py}: Performance evaluation tools
\end{itemize}

\subsection{Parameter Reduction Analysis}

Table \ref{tab:parameters} shows the parameter reduction achieved by different weight sharing schemes for various LDPC codes.

\begin{table}[htbp]
\caption{Parameter Reduction Comparison}
\label{tab:parameters}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Decoder Type} & \textbf{(16200,7200)} & \textbf{(3096,1032)} & \textbf{Reduction} \\
\midrule
N-NMS (No Sharing) & 4.8×10⁵ & 1.6×10⁴ & 1× \\
N-2D-NMS Type 1 & 13 & 41 & 3.7×10⁴ \\
N-2D-NMS Type 2 & 8 & 15 & 6.0×10⁴ \\
N-2D-NMS Type 3 & 4 & 8 & 1.2×10⁵ \\
N-2D-NMS Type 4 & 4 & 7 & 1.2×10⁵ \\
\bottomrule
\end{tabular}
\end{table}

\section{Experimental Results}

\subsection{Simulation Setup}

All simulations were conducted using BPSK modulation over AWGN channels. The following LDPC codes were tested:

\begin{itemize}
\item (7,4) Hamming code for basic testing
\item (16200,7200) DVBS-2 LDPC code
\item (3096,1032) PBRL LDPC code
\end{itemize}

\subsection{Performance Comparison}

Figure \ref{fig:performance} shows the Frame Error Rate (FER) performance comparison for different decoder types.

\begin{figure}[htbp]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{fer_comparison.png}
\caption{FER Performance}
\label{fig:fer}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{ber_comparison.png}
\caption{BER Performance}
\label{fig:ber}
\end{subfigure}
\caption{Performance comparison of different LDPC decoders}
\label{fig:performance}
\end{figure}

\subsection{Gradient Explosion Analysis}

The gradient explosion analysis shows that posterior joint training effectively prevents gradient explosion compared to standard training methods.

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.8\columnwidth]{gradient_analysis.png}
% \caption{Gradient magnitude analysis showing effectiveness of posterior joint training}
% \label{fig:gradient}
% \end{figure}
\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{gradient_analysis.png}
\caption{Gradient magnitude analysis showing effectiveness of posterior joint training.}
\label{fig:gradient}
\end{figure}



\subsection{RCQ Quantization Results}

Table \ref{tab:rcq_results} shows the performance of RCQ decoders with different quantization bit widths.

\begin{table}[htbp]
\caption{RCQ Decoder Performance}
\label{tab:rcq_results}
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Decoder} & \textbf{Bits} & \textbf{FER @ 2dB} & \textbf{Hardware Reduction} \\
\midrule
OMS (baseline) & 5 & 1.2×10⁻³ & 1× \\
RCQ MinSum & 4 & 1.1×10⁻³ & 0.85× \\
W-RCQ Type 2 & 4 & 1.3×10⁻³ & 0.75× \\
RCQ MinSum & 3 & 2.1×10⁻³ & 0.65× \\
W-RCQ Type 2 & 3 & 2.4×10⁻³ & 0.55× \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\subsection{Parameter Efficiency}

The node-degree-based weight sharing schemes achieve significant parameter reduction while maintaining performance. Type 2 sharing provides the best balance between parameter reduction and performance.

\subsection{Hardware Implications}

The reduced parameter count makes neural LDPC decoders feasible for hardware implementation. The RCQ quantization further reduces hardware requirements by enabling low-bitwidth implementations.

\subsection{Gradient Explosion Mitigation}

Posterior joint training effectively addresses gradient explosion issues, enabling stable training of neural LDPC decoders for long-blocklength codes.

\section{Conclusion}

This paper presents a comprehensive implementation of neural LDPC decoders with degree-specific weight sharing and RCQ quantization. The key contributions include:

\begin{itemize}
\item Implementation of Neural MinSum decoder with edge-specific weights
\item Four different node-degree-based weight sharing schemes reducing parameters by 3-4 orders of magnitude
\item Posterior joint training method to address gradient explosion
\item RCQ quantization for low-bitwidth implementations
\item Weighted RCQ decoder combining neural weights with quantization
\item Comprehensive simulation framework for performance evaluation
\end{itemize}

The implementation demonstrates that neural LDPC decoders can achieve comparable performance to traditional decoders while being practical for hardware implementation through parameter reduction and quantization techniques.

\section{Future Work}

Future research directions include:

\begin{itemize}
\item Extension to other LDPC code families
\item Hardware implementation and FPGA synthesis
\item Optimization of quantization parameters
\item Application to other channel models
\item Integration with modern deep learning frameworks
\end{itemize}

\section*{Acknowledgment}

This implementation is based on the work of Linfang Wang, Caleb Terrill, Richard Wesel, and Dariush Divsalar. The authors thank them for their groundbreaking research in neural LDPC decoding.

\begin{thebibliography}{1}

\bibitem{wang2023ldpc}
L. Wang, C. Terrill, R. Wesel, and D. Divsalar, ``LDPC Decoding with Degree-Specific Neural Message Weights and RCQ Decoding,'' \emph{arXiv preprint arXiv:2310.15483v2}, 2023.

\bibitem{gallager1962}
R. G. Gallager, ``Low-density parity-check codes,'' \emph{IRE Trans. Inf. Theory}, vol. 8, no. 1, pp. 21--28, Jan. 1962.

\bibitem{nachmani2016}
E. Nachmani, Y. Be'ery, and D. Burshtein, ``Learning to decode linear codes using deep learning,'' in \emph{Proc. 54th Annu. Allerton Conf. Commun., Control, Comput.}, Sep. 2016, pp. 341--346.

\bibitem{lugosch2017}
L. Lugosch and W. J. Gross, ``Neural offset min-sum decoding,'' in \emph{Proc. IEEE Int. Symp. Inf. Theory (ISIT)}, Jun. 2017, pp. 1361--1365.

\bibitem{wang2021}
L. Wang, S. Chen, J. Nguyen, D. Divsalar, and R. Wesel, ``Neural network-optimized degree-specific weights for LDPC min-sum decoding,'' in \emph{Proc. 11th Int. Symp. Topics Coding (ISTC)}, 2021, pp. 1--5.

\end{thebibliography}

The complete implementation is available on GitHub\footnote{\url{https://github.com/Lalwaniamisha789/Implementation-of-Neural-LDPC-Decoders-with-Degree-Specific-Weight-Sharing-and-RCQ-Quantization}}.

\end{document}
